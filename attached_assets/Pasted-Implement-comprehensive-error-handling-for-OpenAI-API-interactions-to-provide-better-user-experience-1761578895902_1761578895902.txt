Implement comprehensive error handling for OpenAI API interactions
to provide better user experience and system reliability.

ERROR SCENARIOS TO HANDLE:
1. API errors:
- Rate limiting (429)
- Invalid API key (401)
- Server errors (500-599)
- Timeout errors
- Network failures

2. Response errors:
- Incomplete responses
- Malformed JSON
- Content policy violations
- Empty responses
- Unexpected format

3. Streaming errors:
- Stream interrupted mid-response
- Parse errors in SSE stream
- Connection dropped
- Client disconnect

IMPLEMENTATION:
1. Create error handling utilities (server/utils/ai-error-handler.ts):

export class AIError extends Error {
constructor(
message: string,
public code: string,
public statusCode: number,
public retryable: boolean,
public userMessage: string
) {
super(message);
}
}

export function handleOpenAIError(error: any): AIError {
// Rate limiting
if (error.status === 429) {
return new AIError(
'Rate limit exceeded',
'RATE_LIMIT',
429,
true,
'Too many requests. Please wait a moment and try again.'
);
}

// Invalid API key
if (error.status === 401) {
return new AIError(
'Invalid API key',
'AUTH_ERROR',
401,
false,
'Authentication error. Please contact support.'
);
}

// Server errors (retryable)
if (error.status >= 500) {
return new AIError(
'OpenAI server error',
'SERVER_ERROR',
error.status,
true,
'AI service temporarily unavailable. Please try again.'
);
}

// Content policy violation
if (error.status === 400 && error.message?.includes('content_policy')) {
return new AIError(
'Content policy violation',
'CONTENT_POLICY',
400,
false,
'Request violates content policy. Please rephrase your message.'
);
}

// Default
return new AIError(
error.message || 'Unknown error',
'UNKNOWN',
500,
false,
'An unexpected error occurred. Please try again.'
);
}

2. Implement retry logic with exponential backoff:

async function retryWithBackoff<T>(
fn: () => Promise<T>,
maxRetries = 3,
initialDelay = 1000
): Promise<T> {
let lastError: any;

for (let attempt = 0; attempt < maxRetries; attempt++) {
try {
return await fn();
} catch (error) {
const aiError = handleOpenAIError(error);
lastError = aiError;

// Don't retry if not retryable
if (!aiError.retryable || attempt === maxRetries - 1) {
throw aiError;
}

// Exponential backoff
const delay = initialDelay * Math.pow(2, attempt);
await new Promise(resolve => setTimeout(resolve, delay));
}
}

throw lastError;
}

3. Add circuit breaker pattern:

class CircuitBreaker {
private failures = 0;
private lastFailTime = 0;
private state: 'closed' | 'open' | 'half-open' = 'closed';
private readonly threshold = 5;
private readonly timeout = 60000; // 1 minute

async execute<T>(fn: () => Promise<T>): Promise<T> {
if (this.state === 'open') {
if (Date.now() - this.lastFailTime > this.timeout) {
this.state = 'half-open';
} else {
throw new AIError(
'Circuit breaker is open',
'CIRCUIT_OPEN',
503,
true,
'Service temporarily unavailable. Please try again later.'
);
}
}

try {
const result = await fn();
this.onSuccess();
return result;
} catch (error) {
this.onFailure();
throw error;
}
}

private onSuccess() {
this.failures = 0;
this.state = 'closed';
}

private onFailure() {
this.failures++;
this.lastFailTime = Date.now();

if (this.failures >= this.threshold) {
this.state = 'open';
}
}
}

4. Update chat endpoints with error handling:

router.post('/api/chat/message', async (req, res) => {
try {
const result = await circuitBreaker.execute(async () => {
return await retryWithBackoff(async () => {
return await openai.chat.completions.create({
model: 'gpt-4',
messages: req.body.messages,
stream: true
});
});
});

// Stream response...
} catch (error) {
if (error instanceof AIError) {
return res.status(error.statusCode).json({
error: error.userMessage,
code: error.code,
retryable: error.retryable
});
}

// Unexpected error
return res.status(500).json({
error: 'An unexpected error occurred',
code: 'UNKNOWN'
});
}
});

5. Add streaming error recovery:

// In streaming handler
stream.on('error', (error) => {
const aiError = handleOpenAIError(error);

// Send error event to client
res.write(`data: ${JSON.stringify({
error: true,
message: aiError.userMessage,
code: aiError.code,
retryable: aiError.retryable
})}\n\n`);

res.end();
});

FRONTEND ERROR HANDLING:
1. Update chat UI to handle errors gracefully:

// In chat message handler
try {
const response = await fetch('/api/chat/message', {
method: 'POST',
headers: { 'Content-Type': 'application/json' },
body: JSON.stringify({ messages })
});

if (!response.ok) {
const error = await response.json();

if (error.retryable) {
// Show retry button
setError({
message: error.error,
retryable: true,
onRetry: () => sendMessage(message)
});
} else {
// Show permanent error
setError({
message: error.error,
retryable: false
});
}
return;
}

// Process stream...
} catch (error) {
setError({
message: 'Network error. Please check your connection.',
retryable: true,
onRetry: () => sendMessage(message)
});
}

2. Add error UI components:
- Retry button for retryable errors
- Error message display
- Loading state during retry
- Error icon/visual indicator

3. Implement offline detection:
- Detect when user goes offline
- Queue messages for sending when back online
- Show offline indicator
- Prevent sending while offline

MONITORING & LOGGING:
1. Log all AI errors:
- Error type and frequency
- User impact
- Response times
- Success/failure rates

2. Create error dashboard:
- Real-time error monitoring
- Error rate over time
- Most common errors
- User-reported issues

3. Add alerting:
- Alert on high error rates
- Notify on API quota exhaustion
- Alert on circuit breaker trips

TESTING:
- Mock API errors in tests
- Test retry logic with failing calls
- Verify circuit breaker opens/closes correctly
- Test streaming interruption recovery
- Validate user-friendly error messages
- Test offline behavior
